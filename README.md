# CloudAPI

## Dependencies
- .NET 6.0
- Docker(Mac/Windows)
- Docker compose(Mac/Windows)
- MSSQL (Windows)


## Environment variables
There is a `launchSettings.json` file for each project which contains the environment variables.
This is a file specific to each developer(ignored by git), so the developer needs to create their own `lauchSettings.json` file, for each project they want to run.
There is an example of this file in the `Properties` directory in each project.
For running the CloudAPI, only a `launchSettings.json` file in the `CloudAPI/Properties` directory is needed.

### Configuration for the CloudAPI
In the `CloudAPI/Properties/launchSettings.json` file, the environment variables are defined in the `profile.CloudAPI.environmentVariables` object.
Environment variables that are required at minimum for the CloudAPI are marked with an asterix "*".

- *UseAzureAppConfig
In case the CloudAPI is not supposed to fetch dynamic App Configuration at run-time, set UseAzureAppConfig to false.
This could be used for customers not wanting to reference external app config from a cloud based Azure config storage.
So only use UseAzureAppConfig = "true" in case config needs to be fetched from a cloud Azure Portal.

- *TENANTS
key-value pair of `tenantID` and a json object with two key-value pairs: the `connectionString` and the `licensekey`. See *Add database connection strings*

- *ASPNETCORE_ENVIRONMENT
This value specifies which appsettings file the compiler needs to use when you run the project.
When running locally use 'Development'.

- *ConnectionStrings:Database
Value of the key-value pair as in the TENANTS parameter. This is used to set up the db connection at start
of the CloudAPI application.

- *ConnectionStrings:SecurityDatabase 
The connection string for the security database. Security database contains the data protection keys.

Information on Log levels in ASP.NET Core 6.0: https://docs.microsoft.com/en-us/aspnet/core/fundamentals/logging/?view=aspnetcore-6.0
- *Logging:LogLevel:Default
Application logger level. Use "Trace" value for additional information required for debugging.

- *Logging:LogLevel:Microsoft
Logs all Microsoft categories. Set to "Warning" for Warning level logs and higher.

- *Logging:LogLevel:Microsoft.Hosting.Lifetime
Logs all Microsoft.Hosting.Lifetime categories. Set to "Information" for Information level logs and higher.

- *ReplicationMode
Indicates whether or not to use application replication. Values supported by application are "OFF" or "REDIS".
Value should be set "OFF" to use default SignalR connection without replication using Redis.

- *Jwt:CabinetAPIKey
API key recognized by CloudAPI as valid. The cabinet authenticates using this specific key for further requests.

- *Jwt:PrivateKey
Private key used to encode sensitive information such as passwords.

- *Jwt:PublicKey
Public key used to setup the RSA security key.

- *Jwt:Issuer
Identifies principal that issued the JWT. Should be the same as value of ASPNETCORE_URLS environment variable.
Should be the same as value of ASPNETCORE_URLS environment variable. "http://localhost:5001" used for local development.

- *Jwt:Audience
Identifies the recipients that the JWT is intended for. Each principal intended to process the JWT must identify itself with a value in the audience claim. 
If the principal processing the claim does not identify itself with a value in the aud claim when this claim is present, then the JWT must be rejected.
Should be the same as value of ASPNETCORE_URLS environment variable. "http://localhost:5001" used for local development.

- *Jwt:AccessTokenValidityInSeconds
This environment variable controls the lifespan of JWT access tokens generated by the application. The value is defined in seconds and dictates how long an access token remains valid after being issued. It's crucial for the security of the application, ensuring that tokens are periodically refreshed and not left valid indefinitely.

- *ASPNETCORE_URLS
By default if omitted "http://localhost:5000". Indicates the IP addresses or host addresses with ports and protocols 
that the server should listen on for requests. Multiple could be added in one string by seperating with ";".
For local development "http://localhost:5001" should be used.


- AllowedHosts
Host Filtering Middleware, disabled by default. Any host other than an explicit IP address binds to all public IP addresses.

- AzureSignalRKey
Adds the minimum essential Azure SignalR service. Omit this environment variable when no Azure SignalR service is used. 
In case it is used; specify the endpoint of the service, the access key and version being used. E.g. Endpoint="https://dummy.service.signalr.net;AccessKey=123456;Version=1.0;"

- ID
To identify the server, this ID is send to clients connected via SignalR

- MailFromAddress
This variable specifies the email address displayed in the "From" field of outgoing emails, identifying the sender to recipients and helping to prevent emails from being marked as spam. For SendGrid, only use in sendgrid verified email addresses to not get rejected by the API.

- SendGridAPIKey
This variable specifies the API key for SendGrid, a preferred email sending service that is used when available. It's applied when the tenant-specific configuration is not found, ensuring a reliable email sending option is always at hand.


- SmtpHost, SmtpPort, SmtpUsername, SmtpPassword, SmtpFromAddress (used only if MailFromAddress is not set)
These variables define the SMTP server details for email delivery. They are used if the SMTP strategy is determined to be the appropriate fallback.

- EmailMaxRetries
Defines the maximum number of attempts the system should make to send an email before marking it as failed. This ensures robust handling of temporary delivery issues, enhancing the reliability of email communications.


#### Configuration Order and Fallback Mechanism mail sending
When determining email settings, the system follows a specific order:
1. **SendGridAPIKey** from the CTAMSetting table in tenant's database for SendGrid strategy.
2. **SMTP Configuration settings** from the CTAMSetting table in tenant's database if no SendGrid API Key is found.
3. **SendGridAPIKey** from environment variables as a fallback.
4. **SMTP Configuration settings** from environment variables as the final fallback.


#### Add database connection strings
Add new file `CloudAPI/appsettings.Development.json` which is the copy of `CloudAPI/appsettings.Development.Example.json` and modify its `Database` value to your local database connection string.
This will be used during migration to fill the database.

Next create another file `CloudAPI/Properties/lauchSettings.json` which is the copy of `CloudAPI/Properties/lauchSettings.json.example` but it should have `TENANTS` value under the `CloudAPI` > `environmentVariables`.
`TENANTS` is a json object in a string format having `tenantID` as keys and json objects with keys `connectionString` and `licensekey` as values e.g.:
```
...
  "CloudAPI": {
    ...
    "environmentVariables": {
      ... 
      "TENANTS": "{'dummyTenantID1':{'connectionString':'Server=localhost;Database=ctam;User ID=dummyUserName1;Password=dummyPassword1;Application Name=dummyCustomerName1.CloudAPI','licensekey':'lic1'},'dummyTenantID2':{'Server=localhost;Database=ctam;User ID=sa;Password=dummyPassword2;Application Name=dummyCustomerName2.CloudAPI','licensekey':'lic2'}}"
    }
  }
...
```
This will be used by all API calls to make tenant dependent database connection.
  **please note** : 'dummyTenantID1' and 'dummyTenantID2' are required values. Not 'foo' 'bar'.

## Setup Database (Windows)
1. Create 'CTAM' database on your local MS-SQL server.
2. Get connection string. You need to apply this in the lauchSettings.json and appsettings.json as desbribed.
  In Visual Studio, open server explorer (Ctrl W + L) or create it manually.
3. Run Migrations:
      - Open package manager console: (select CloudAPI as default project)
      - Check you have tooling installed: dotnet-ef. 
      ( If not -> install by command: dotnet tool install --global dotnet-ef )
      - Execute command: dotnet-ef database update
      - On error: no project selected -> Open powershell in root folder CloudApi project.
      -> execute command: dotnet-ef database update
      - On error: More than one DBContext. -> Execute command: dotnet-ef database update --context MainDbContext

Check your local database. Tables should be there.

## Setup Database (MAC)
Set the `ASPNETENVIRONMENT` variable to `Development` when using the local database
1. Right-click on the CloudAPI project in the right menu and click on `Options`
2. In `Run -> Configurations -> Default` change the value to `Development`

### Create database for the first time
Start your docker desktop first after that run the following commands:
```
cd scripts
./create_database.sh
```
This will download and create a Docker image for Microsoft SQL server 2019
It will create a volume called `mssql-volume` and attach it to the image.
After this step follow the instructions under `Run migrations` to setup the tables in the database

### Reset the database
Run the `reset_database.sh` to remove the mssql docker container and the volume attached to this container.
This will delete all data and tables in the CTAM database. Run the `create_database.sh` after this if you want a new database.

### Run the database
```
docker start CTAM
```
### Stop the database
```
docker stop CTAM
```
## Full Development Docker Setup
For development purposes it is very convenient to run one of the or all projects in Docker. This makes development of a single project easier as you can run all other projects in docker 

**Make sure the 'applicationUrl' in `launchSettings.json` in CTAM_CLOUAPI/CloudAPI/Properties/  is set to `http://localhost:5001` if you want to use the full docker setup where the CloudAPI project is run in VisualStudio**

### Step 1: Run docker-compose 
Setup for CTAM (ctamclouddb, ctamcloudapi, ctamlocalui, ctamlocalapi, ctamhardwareapi, ctamlocaldb, ctamlocalui)
 
 
This relies on docker images of `ctamclouddb`, `ctamcloudapi`, `ctamlocalapi`, `ctamhardwareapi`, `ctamlocaldb` and `ctamlocalui`. These images will be pulled from the cloud container registry: `ctrdeuwctamdevcontregistry.azurecr.io`.  
These images contain the latest version of the source code on the develop branch.
To access this container registry in the cloud we have to authenticate ourselves through the `AZ CLI`

```
// opens a tab to log you into azure CLI
az login 

// authenticates you to the container
az acr login --name ctrdeuwctamdevcontregistry -u ctrdeuwctamdevcontregistry -p SxWLsw3cZ75eWA/Ibuu8nAoHnizgOoQp
```
**Username and password**

Username and password may be different, if either has changed you can find the new credentials using this path: 
1. Go to portal.azure.net
2. Search for recoursegroup CT-RD-EUW-CTAMDEV-CONT-RG
3. Under recourses go to ctrdeuwctamdevcontregistry
4. On the left, go to "Acces keys" or "Toegangssleutels"
Here you will find the username and/or password  

**To pull newest images for the single cabinet setup run this in your `CTAM_CloudAPI` folder.**
```
docker-compose pull
```

**To pull newest images for the dual cabinet setup run this in your `CTAM_CloudAPI` folder.**
```
docker-compose -f docker-compose-ctam-dual-cabinet.yml pull
```

**To run the single cabinet setup run this in your `CTAM_CloudAPI` folder.**
```
docker-compose -p ctam up -d
```

**To run the dual cabinet setup run this in your `CTAM_CloudAPI` folder.**
```
docker-compose -f docker-compose-ctam-dual-cabinet.yml -p ctam-dual-cabinet up -d
```



To manage which containers are run you can use the Docker GUI or use commands below to stop a single container, which you can then develop upon in VisualStudio.
```
docker stop [ctamcloudapi, ctamclouddb, ctamcloudui, ctamlocalapi, ctamhardwareapi, ctamlocalui] 

docker start [ctamcloudapi, ctamclouddb, ctamcloudui, ctamlocalapi, ctamhardwareapi, ctamlocalui]

docker stop $(docker ps -aq) // stop all containers

docker rm $(docker ps -aq) // remove all containers

docker volume prune // remove all non-active volumes, volumes are used to persist database changes.

docker image prune // remove all dangling images
```

Alternatively you could only run the images `ctamcloudapi`, `ctamclouddb` and `ctamcloudui`.

To create a connection between multiple docker containers we need to create a network `ctam_network` in docker.
```
docker network create ctam_network
```
This application will be linked to the `ctam_network` that was created.
```
docker-compose -f docker-compose-ctam-cloud-only up -d
```
### Step 2: Create local dockerimages (Optional)
This will build your local source code into docker images to replace the docker images from the cloud on your local machine.

**run this in your `CTAM_CloudAPI` folder.**
```
docker build -t ctrdeuwctamdevcontregistry.azurecr.io/ctamcloudapi .
```
**run this in your `CTAM_CloudUI` folder.**
```
docker build -t ctrdeuwctamdevcontregistry.azurecr.io/ctamcloudui .
```
**run this in your `CTAM_LocalAPI` folder.**
```
docker build -t ctrdeuwctamdevcontregistry.azurecr.io/ctamlocalapi -f Dockerfile.local .
```
**run this in your `CTAM_HardwareAPI/CTAM_HardwareAPI` folder.**
```
docker build -t ctrdeuwctamdevcontregistry.azurecr.io/ctamhardwareapi -f Dockerfile.local .
```
**run this in your `CTAM_LocalUI` folder.**
```
docker build -t ctrdeuwctamdevcontregistry.azurecr.io/ctamlocalui .
```

# CTAMSeeder

## using ctamseeder tool to seed database
Seeding a database with production or development data will always clean the database you are seeding. Seeding a database with dummy data will add only non-existing dummy data to the database.

**run this in your `CTAMSeeder` folder to seed the database with data or clean the development database**
```
dotnet run -- [seed/clean] [productionData/ProductionDataBR/DevelopmentData/DummyData]
```

**run this in your `CTAMSeeder` folder to seed or migrate a cloud database to the latest migration**
```
dotnet run -- \
  -db <connectionstring> \
  -bc <blob-connection> \
  -bcn <blob-container-name> \
  -bn <blob-name> \
  -kv <keyvault-identifier> \
  [acceptance/developmentCloud] \
  [seed/migrate] \
  [productionData/ProductionDataBR/DevelopmentData]
```
When receiving the following error ensure that your ip is authorized on the keyvault and database in portal.azure.com for the affected resources in the cloud.
```
Client address is not authorized and caller is not a trusted service.
```
****
# Migrations

## Run migrations
This will create the tables and process changes in the model to the database.

Run these commands inside the project in CTAM_CloudAPI/CloudAPI to create the CTAM and CTAM-Security database.
```
dotnet ef database update --context MainDbContext
```
```
dotnet ef database update --context SecurityDbContext
```
If this gives an error about Parameter UseAzureAppConfig not defined execute:

### On Windows:
```
$env:UseAzureAppConfig = 'false'
```

### On Mac: 

```
export UseAzureAppConfig=false
```
## Add migration
Changes to the entities need to be migrated to the database, use the following command to create a new migration
```
dotnet ef migrations add <name> --context MainDbContext
```

```
Add-Migration InitialCreate -Context MainDbContext
```

If this gives an error about Parameter UseAzureAppConfig not defined execute:

### On Windows:
```
$env:UseAzureAppConfig = 'false'
```

### On Mac: 

```
export UseAzureAppConfig=false
```
After this step follow the instructions under `Run migrations` to execute the migration

*If you encounter following error:*
```
An error occurred while accessing the Microsoft.Extensions.Hosting services. Continuing without the application service provider. Error: Value cannot be null. (Parameter 'Environment variable CONNECTION_STRING is not defined')
```
then execute the following command in terminal(UNIX) `export CONNECTION_STRING=<connection_string>`

## Fix migration order
It could happen by accident that one of the developers merges a pull request with a new migration that has a timestamp later compared to another ongoing task/PR that has a migration with a timestamp created before.
In this case, make sure that the ongoing task/PR updates the timestamp in its file name and designer class name, so that EF recognizes the correct order in which the migrations have to be applied to the database.

If both the migrations are already applied to the database, the approach to fix this is by changing the timestamp of the second migration(with timestamp that comes before the first migration) in the EFMigrationsHistory
table. Afterwards, this should be fixed in the next release by changing the timestamp in code.

## Run application
After running the migration start the application in visual studio and visit localhost:5001.
Execute a GET request to test if the database is connected. If this returns status 200 the connection succeeded.

## Drop database script
```sql
USE master;
GO

ALTER DATABASE ctam SET SINGLE_USER WITH ROLLBACK IMMEDIATE;
GO
DROP DATABASE ctam;
GO
```

## Drop all tables script (MSSQL)
```sql
USE  ctam

DECLARE @sqlDrop NVARCHAR(max)=''

SELECT @sqlDrop += ' Drop table ' + QUOTENAME(TABLE_SCHEMA) + '.'+ QUOTENAME(TABLE_NAME) + '; '
FROM   INFORMATION_SCHEMA.TABLES
WHERE  TABLE_TYPE = 'BASE TABLE'

Exec Sp_executesql @sqlDrop;
Exec Sp_executesql @sqlDrop;
Exec Sp_executesql @sqlDrop;
```

## Simulating mail
To test mail functionality locally in an emulator you can use mailhog
Go in Docker - Search - "mailhog/mailhog" - pull
In Images you can run mailhog/mailhog and set the Optional settings, or in PowerShell:
```
docker run --name=mailhog -p 25:1025 -p 8025:8025 mailhog/mailhog
```

Now you can send mail to localhost:25 in your code and see the sent mails in the browser in http://localhost:8025. 



## Creating new module
* In VS create new `Web and Console > App> API` project
* Remove unnecessary auto generated files and folders e.g. `WeatherForecast.cs`, Controllers folder
* Add a parent module(s) as a reference to the project `Dependencies`
* Compare `appsettings.json` and all underlying settings(e.g. `appsettings.Development.json`) from previous modules with the generated one in new moduleand modify if necessary
* Do the same for `Properties/launchSettings.json`, do not forget to change environmentVariable ID according to your new  module name
* Repeat the process for `Startup.cs` file, but in this case `ConfigureServices` method should be equal to the parent one
* Copy `<module_name>StartupServices.cs` file from parent module and modify for your needs
  * `StartupServices` and `DbContext` classes are using composite design pattern to effectively inherit functionality from multiple imported projects
* Copy `Program.cs` class content from parent module and modify for your needs
* In all copied files change the namespaces to your new module namespace
* After you finished implementing `DbContext` for the new module run `dotnet ef migrations add <migration name>  --context <name of new module DbContext>`
* in VS `Add-Migration InitialCreate -Context MyDbContext`
* Before running the new module make sure that there are no tables in the database
* To run new module press with right mouse on the project and click `Run Project`

### Integration activation
The CloudAPI supports integrating dynamically with external APIs. Some of the integrations are triggered by e.g. actions on the cabinet.
The cabinet related triggers can cause an API call to be executed with an external API/system. But each of the external API might have its own specification.
Hence, we support a dynamic way of creating a REST request partially filled in by a table called the APISetting.

The current available API trigger names caused by actions on the cabinet are:
- SEND_DEFECT_ON_SWAP
- SEND_REPAIRED
- SEND_REPLACED

The current avaiable API trigger names that indicate a pre-request for authenticating on e.g. a SEND_DEFECT_ON_SWAP are:
- OAUTH2_CLIENT_CREDENTIALS

Utilizing these integrations requires filling in the placeholders of the APISetting records and activating it.
Here is an example of how to activate the APISetting that is responsible for executing a REST request to the external API
triggered by the TriggerName 'SEND_DEFECT_ON_SWAP':
```sql
UPDATE Communication.APISetting
SET 
RequestT = 'CloudAPI.Integrations.ExampleClientTenantID.Requests.SwapRequest',
ResponseT = 'CloudAPI.Integrations.ExampleCLientTenantID.Responses.SwapResponse',
API_URL = 'https://api.ext.example/incidents/v1',
API_HEADERS = '{"key1":"value1"}',
CrudOperation = 1,
Active = 1,
IntegrationSystem = 'Example Client Name',
AuthenticationTriggerName = 'OAUTH2_CLIENT_CREDENTIALS',
HasAuthentication = 1,
API_BODY = '{"key1": "value1", "key2": "value2"}'
WHERE TriggerName = 'SEND_DEFECT_ON_SWAP';

UPDATE Communication.APISetting
SET 
API_URL = 'https://api.ext.example/auth/oauth/v1/token',
API_HEADERS = '{"Content-Type": "application/x-www-form-urlencoded"}',
CrudOperation = 1,
Active = 1,
IntegrationSystem = 'Example Client Name',
AuthenticationTriggerName = '',
HasAuthentication = 0,
API_BODY = '{"grant_type":"client_credentials","client_id":"123456789", "client_secret":"123456789", "scope":"this_is_optional"}'
WHERE TriggerName = 'OAUTH2_CLIENT_CREDENTIALS';
```

For all APISetting records(in case the client activates it by setting Active = 1), the following columns should be filled:
- RequestT: in communication with the development team
- ResponseT: in communication with the development team
- API_URL: the url of the external API to request using REST. Should be filled in by the client.
- API_HEADERS: see the above example for the format. The headers in this column are given to the REST request.
- CrudOperation: 0 = GET, 1 = POST, 2 = PUT, 3 = DELETE. Dependent on the REST specification of the API_URL.
- Active: 0 = inactive, 1 = active
- IntegrationSystem: name of the client. This only for administration purposes and will not have impact on the integration success.
- AuthenticationTriggerName: Which authentication method to use if the APISetting needs a pre-request to retrieve authentication details.
- HasAuthentication: 0 = inactive, 1 = active
- API_BODY: see the above example for the format. The body fields will overwrite programmatic fields in the RequestT.